Celem wdrażania mechanizmów wysokiej dostępności (High Availability) jest niedopuszczenie do przestoju kluczowych procesów biznesowych przedsiębiorstwa lub – jeżeli awaria nastąpi – zminimalizowanie ich wpływu na działanie organizacji. Zapewnienie wysokiej dostępności jest szczególnie istotne dla firm, których biznes zależy od utrzymywania stałego dostępu do usług dla klientów (np. serwisy online,platformy muzyczne i VOD, banki), lub organizacji, gdzie przerwanie ciągłości pracy skutkuje wysokimi kosztami jej ponownego uruchomienia (np. przemysł, produkcja, wydobycie).
Zagwarantowanie nieprzerwanego działania infrastruktury IT jest przy tym dużym wyzwaniem, ponieważ tego typu projekty muszą obejmować wiele obszarów: od usunięcia możliwości wystąpienia pojedynczego punktu awarii (serwera, węzła pamięci masowej, urządzenia sieciowego, aplikacji itd.), poprzez zapewnienie redundantnych ścieżek dostępu do sieci i równomiernego rozłożenia ruchu, aż po możliwość błyskawicznego odzyskania krytycznych danych oraz dostępu do kluczowych aplikacji biznesowych po wystąpieniu awarii. Od tego zależeć będzie, czy (na ile) firmowa architektura IT będzie odporna na awarie czy spadki wydajności. W oczywisty sposób im większy stopień jej skomplikowania, tym zapewnienie wysokiej dostępności będzie trudniejsze.
Żadna aplikacja nie może działać prawidłowo, gdy nie ma dostępu do zasilających ją danych, dlategoważnym elementem strategii w obszarze gwarantowania wysokiej dostępności firmowych aplikacji jest sposób ich składowania i przetwarzania. Strategia składowania danych będzie wpływała zarówno na wydajność, poziom zabezpieczeń oraz topologię, jak i system plików, sposoby dostępu czy metody zapewnienia zgodności z wymaganiami ustawodawcy.
Dane on-site, w chmurze czy pomiędzy?
W przypadku składowania danych w środowisku on-premise ważne jest ustalenie newralgicznych dla prawidłowego działania zaplecza IT punktóww architekturze informatycznej firmy. W grę wchodzą wirtualizacja i rozwiązania klastrowe, równoważenie obciążenia, zapewnianie dostępu do sieci itd.
Zagwarantowanie wysokiej dostępności danych po stronie własnej infrastruktury IT przekłada się naznaczne koszty dublowania serwerów,macierzy dyskowych itd. Nakłady te będą istotne już w przypadku budowania środowiska Active-Passive, a zwłaszcza Active-Active. Z punktu widzenia dostępności należy pamiętać, że tradycyjne środowiska NAS są zazwyczaj instalowane w pojedynczej lokalizacji (nawet jeżeli uwzględnione zostały redundantne elementy infrastruktury), co może przełożyć się na opóźnienia w odpowiedzi na zapytania wysyłane przez użytkowników znajdujących się daleko od centrum danych.
Hierarchiczne systemy plików doskonale nadają się do składowania i przetwarzania danych w sieciach lokalnych, gdzie wzrost wolumenu danych nie będzie odbywał się w sposób wykładniczy. Tradycyjny storage pozostaje niezastąpiony tam, gdzie kluczowy jest czas opóźnień w operacjach wejścia–wyjścia (I/O).
Skalowanie lokalnych rozwiązań typu NAS możliwe jest w jednym z dwóch modeli: pionowym (scale-up) oraz poziomym (scale-out). Pierwszy oznacza inwestycję w nowe, charakteryzujące się większą mocą obliczeniową czy przestrzenią dyskową serwery; drugi dodawanie do istniejącego zaplecza nowych urządzeń, między które rozdzielany będzie wzrastający ruch czy rosnący strumień danych . Oba modele generują koszty; scale-up skokowo, wraz z wymianą sprzętu na wydajniejsze odpowiedniki nowej generacji. Scale-out umożliwia zapanowanie nad kosztami poprzez sukcesywne zwiększanie nakładów wraz ze wzrostem zapotrzebowania i dodawaniem kolejnych węzłów. Wadą tej metody jest wyższy koszt rozbudowy w zestawieniu ze zwiększaniem zasobów w ramach tego samego urządzenia skalowanego w górę.
Zaletą posiadania własnej serwerowni czy centrum danych jest możliwość zapewnienia (przynajmniej w teorii) pełnej kontroli nad danymi, niezależnie od ich typu. To ułatwia spełnienie wymagań regulatora, zwłaszcza w regulowanych w dużym stopniu branżach, takich jak bankowość czy finanse.
W obszarze składowania danych szybko wzrasta popularność chmury obliczeniowej. Tego typu usługi oferuje wielu dostawców, zarówno polskich, jak i zagranicznych. Konkurencyjne ceny wykorzystania przestrzeni dyskowej (przeważnie w euro lub dolarach per 1 GB) i niska bariera wejścia sprawiają, że przedsiębiorstwa coraz chętniej sięgają po tę wygodną formę składowania danych. Co istotne, chmurę nieporównywalnie łatwiej skalować niż w przypadku środowisk on-premise.
Biorąc pod uwagę zapewnienie wysokiej dostępności,przeniesienie danych w chmurę oznacza przesunięcie akcentów z zapewniania redundancji własnych macierzy dyskowych na rzeczdostępności łączy o wysokiej przepustowości.W tym modelu przedsiębiorstwo jest bowiem zależne od możliwości skomunikowania się z usługodawcą oraz niezawodności oferowanych przez niego usług dostępowych czy chmurowych. Choć wyspecjalizowani dostawcy rozwiązań cloud computing mają zazwyczaj znacznie bardziej rozbudowane mechanizmy wysokiej dostępności niż ich klienci, to im także zdarzają się przestoje. Przykładem sąniedawne problemy z dostępnością sieci szkieletowej OVH czy zeszłoroczne ataki na Dyn, amerykańskiego dostawcę usług chmurowych i DNS.
Wykorzystanie chmury obliczeniowej do składowania danych wymaga, by pamiętać o elementach ryzyka. Chodzi np.o to, jakie są narzucane przez regulatora bądź politykę firmy ograniczenia co do rodzaju danych gromadzonych w chmurze? Jakie SLA – i w jakiej cenie – oferuje dostawca chmury obliczeniowej? Czy konieczna będzie konwersja danych i dostosowanie posiadanych aplikacji do obsługi API udostępnianego przez danego usługodawcę? Jakie będą realne koszty przejścia w chmurę, skoro wiele polskich przedsiębiorstw ma problemy z oszacowaniem nawet tego, jakie są całkowite koszty utrzymania danych po stronie ich własnej infrastruktury (sprzęt i amortyzacja, praca specjalistów, biura, klimatyzacja, dostęp do sieci etc.)? Nie mając ustalonego punktu wyjścia, trudno ustosunkować się do oferty dostawców.
Wspomniane wyżej kwestie sprawiają, że chętnie rozważaną przez działy IT opcją stają się środowiska hybrydowe, łączące wygodę i elastyczność chmury publicznej z daleko posuniętą możliwością panowania nad firmowymi danymi, charakterystyczną dla środowisk lokalnych.
W ten sposób za pomocą różnego rodzaju warstw pośredniczących przedsiębiorstwa spajają ze sobą zarówno tradycyjną infrastrukturę lokalną, chmury prywatne, jak i usługi oferowane w chmurze publicznej. To właśnie w tym obszarze widzą przyszłość najwięksi dostawcy rozwiązań IT. Przykładowo VMware udostępniło usługę Cloud on AWS: chmurę publiczną, na którą klienci firmy mogą łatwo przenieść wybrane aplikacje on-premise bez konieczności ich modyfikowania. HPE oferuje platformę Synergy, umożliwiającą dostarczanie zasobów obliczeniowych, pamięciowych i sieciowych jako elementów jednej, programowalnej puli.
Obiektowe pamięci masowe sprawdzają się w przypadku danych statycznych lub przy składowaniu w chmurze obliczeniowej. Object storage znakomicie nadaje się do strumieniowego udostępniania materiałów dźwiękowych lub wideo czy archiwizacji, backupu i odzyskiwania danych po awarii.
„Tradycyjna infrastruktura IT jest zbyt statyczna – przygotowanie i wdrożenie nowej aplikacji lub zmian w usługach może w tradycyjnym modelu zająć tygodnie, a nawet miesiące. HPE Synergy wychodzi naprzeciw temu problemowi; zawiera komplet zasobów IT i wbudowaną inteligencję, pozwalającą za pomocą jednego interfejsu błyskawicznie i automatycznie łączyć te komponenty w optymalnie skonfigurowaną infrastrukturę dla naszych aplikacji. Co więcej, komponować infrastrukturę może zarówno administrator IT, jak i deweloper aplikacji, gdyż Synergy udostępnia uniwersalny i otwarty interfejs zarządzania dla wszystkich” – mówi Marek Bzura, architekt rozwiązań klasy Enterprise w HPE Polska.
Dojrzałe rozwiązania programowe w obszarze składowania danych (Software Defined Storage) pozwalają na tworzenie wydajnych środowisk hybrydowych łączących zarówno elementy lokalne, jak i wiele różnych chmur obliczeniowych od różnych dostawców. Taką ofertę adresuje partner HPE, firma Scality. Platforma Scality Ring wraz z narzędziem Zenko umożliwia m.in. wygodne połączenie zarówno własnej pamięci masowej on-site, jak i wielu różnych środowisk chmurowych: Amazon S3, Google Cloud czy Azure. Wykorzystując interfejs programistyczny (API) popularnego protokołu S3, oprogramowanie to automatycznie konwertuje wysyłane przez wybrane aplikacje dane do standardów obowiązujących w wymienionych platformach chmurowych. Tego typu rozwiązanie pozwala np. wypełniać wymogi regulatorów , umożliwiając łatwe zarządzanie strumieniami danych wysyłanymi do określonych lokalizacji z uwzględnieniem przepisów dotyczących poufności. Z punktu widzenia projektów typu High Availability/Disaster Recovery dodatkową zaletą jest możliwość zbudowania mechanizmu krzyżowego tworzenia kopii zapasowych lub wykorzystanie w tym celu przestrzeni dyskowej oferowanej przez różnych dostawców chmur, a przede wszystkim stworzenie środowiska Active-Active dla wielu lokalizacji opartych na własnych centrach danych przedsiębiorstwa. „Wymaga to oczywiście spełnienia pewnych wymagań dotyczących parametrów łącza pomiędzy lokalizacjami, niemniej natywne mechanizmy georeplikacji Scality Ring pozwalają na zbudowanie rozciągniętego geograficznie, aktywnego storage’owego klastra z pojedynczą przestrzenią nazw. Daje to istotną przewagę w stosunku do tradycyjnych rozwiązań NAS” – mówi Dariusz Świderski, członek zarządu Megatel, krakowskiego integratora klastrów obliczeniowych.
Sposoby organizacji danych a wysoka dostępność
Najszerzej znanym modelem składowania danych jest tradycyjny, hierarchiczny system plików. Skalowalne systemy plików (Network Attached Storage) prezentują dane w strukturze drzew katalogowych, rozgałęziających się na kolejne podkatalogi. Zaletami takiej struktury, z perspektywy użytkownika przypominającej serwer plików, są czytelność i przejrzystość. Hierarchiczna systematyzacja plików ma jednak wady i ograniczenia. Przy szybko wzrastającej liczbie plików i wolumenów danych drzewo katalogowe rozrasta się, wraz z czasem coraz bardziej ograniczając wydajność systemu przy wyszukiwaniu plików. Aby tego uniknąć, konieczne są nakłady inwestycyjne w skalowanie poziome lub pionowe.Hierarchiczne modele zarządzania danymi sprawiają trudności także w przypadku mieszanych katalogów, które składają się zarówno z wielu bardzo małych (liczących po kilka kB), jak i dużych plików (od kilku do kilkanastu GB).
W tradycyjnej architekturze składowania danych wykorzystuje się macierze RAID: technologię magazynowania danych umożliwiającą łączenie wielu dysków twardych w logiczną całość. W takim modelu przekazywane przez aplikacje dane są dzielone na części, a następnie rozprowadzane między wszystkie dyski składające się na dany RAID.
Hierarchiczne systemy plików doskonale nadają się do składowania i przetwarzania danych w sieciach lokalnych, gdzie wzrost wolumenu danych nie będzie odbywał się w sposób wykładniczy (tak jak w przypadku wielkich danych z sieci społecznościowych czy czujników). Tradycyjny storage wciąż pozostaje niezastąpiony tam, gdzie kluczowy jest czas opóźnień w operacjach wejścia–wyjścia (I/O), a więc np. do takich zastosowań, jak dane transakcyjne czy współdzielenie plików. Funkcjonalność tradycyjnego, blokowego modelu przetwarzania danych można uzyskać także w chmurze obliczeniowej (oferty typu cloud Block storage).
Alternatywą jest obiektowy model składowania danych, w którym dane przechowywane są w formie obiektów niemających hierarchicznej, katalogowej struktury. Dane odczytywane są za pomocą unikatowego numeru ID, który umożliwia szybką identyfikację poszukiwanego obiektu. Zaletą tego modelu jest wysoka wydajność, niezależnie od wolumenu danych czy różnic w wielkości zapisywanych plików. Object storage umożliwia wzbogacenie danych o rozbudowane metadane, ułatwiające użytkownikom ich wygodne odszukiwanie. Jednocześnie płaska struktura sprawia, że pamięć obiektowa bardzo łatwo się skaluje; jest też łatwa w zarządzaniu – nie trzeba znać lokalizacji danych, wystarczy obiektu.
Obiektowe pamięci masowe doskonale nadają się do zastosowania w przypadku danych statycznych (rzadko zmienianych) lub do składowania w chmurze obliczeniowej. Object storage, taki jak Scality RING, znakomicie nadaje się do zastosowania np. w systemach do strumieniowego udostępniania materiałów dźwiękowych, wideo, archiwizacji, backupu czy odzyskiwania danych po awarii. Pamięć obiektowa może doskonale uzupełniać tradycyjne systemy blokowe, zwłaszcza w zakresie przechowywania i udostępniania niestrukturyzowanych danych, które obecnie stanowią już ponad 80% wszystkich danych generowanych przez użytkowników systemów informatycznych.
Replikacja czy algorytmy rozpraszające
W tradycyjnej, najczęściej spotykanej architekturze składowania danych wykorzystuje się macierze RAID: technologię magazynowania danych umożliwiającą łączenie wielu dysków twardych w logiczną całość. W takim modelu przekazywane przez aplikacje dane są dzielone (paskowane) na części (bloki), a następnie rozprowadzane między wszystkie dyski składające się na dany RAID. Dzięki równolegle przeprowadzanym operacjom procesy zapisu i odczytu są bardzo wydajne, a z perspektywy użytkownika system prezentuje się spójnie i zrozumiale.
Wadą takiej architektury jest brak odporności na awarię dysków. Aby jej uniknąć, firmy inwestują w kolejne grupy dyskowe, gdzie utrzymywane są kopie zapasowe. Wraz z przyrostem danych oraz rozszerzaniem dostępnej pamięci masowej rośnie niestety także prawdopodobieństwo wystąpienia awarii któregoś z kolejnych dysków, co zwiększa zapotrzebowanie na dalsze replikowanie danych. W ten sposób duże wolumeny składowanych danych przekładają się na jeszcze większe koszty. Zapewnienie wysokiej dostępności wiąże się bowiem z koniecznością równoległego utrzymywania kilku kopii tych samych danych – oznacza to dodatkowe wydatki na sprzęt, prąd, chłodzenie czy pracę specjalistów.
Alternatywą dla replikacji wykorzystywanej w tradycyjnych macierzach RAID jest stosowane w obiektowych pamięciach masowych tzw. kodowanie parzystości (Erasure Coding). Mechanizm ten umożliwia uzyskanie podobnego stopnia zabezpieczenia danych jak w przypadku klasycznej replikacji, ale przy lepszym wykorzystaniu dostępnej przestrzeni dyskowej, poprzez zoptymalizowanie wykorzystania tzw. pamięci surowej (RAW) w stosunku do pamięci użytkowej.W tym modelu gromadzone dane przetwarzane są przez algorytm, który rozprasza je pomiędzy znajdujące się w sieci węzły pamięci. Rozproszenie to umożliwia ustalenie pożądanego poziomu niezawodności i dostępności dzięki określeniu minimalnej liczby elementów zapisu, która będzie niezbędna, aby możliwe było odzyskanie bezstratnego zapisu. Podkreślić należy, że kodowanie parzystości nie polega na tworzeniu redundantnych kopii (tak jak w przypadku replikacji), ale na rozpraszaniu jednej, podstawowej kopii. Zmniejsza to zapotrzebowanie na dodatkową infrastrukturę pasywną do mechanizmów High Availability czy Disaster Recovery, pozwalając wykorzystać ją do innych zadań. Erasure Coding stosowany jest w obiektowych pamięciach masowych zarówno w chmurze obliczeniowej, jak i na wybranych platformach (np. Scality Ring) w środowiskach lokalnych.
Dla jeszcze lepszych efektów mechanizmy replikacji oraz kodowania parzystości mogą się także uzupełniać. „Oba mechanizmy oraz georeplikacja pozwalają na zbudowanie środowiska praktycznie odpornego na dowolny typ awarii: poszczególnych centrów danych, całych szaf RACK czy kilku węzłów storage na raz. Bez utraty i przerw w dostępie do danych" – podkreśla Dariusz Świderski.

